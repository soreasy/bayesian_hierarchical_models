{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e8da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "0f3371d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zone_id</th>\n",
       "      <th>date</th>\n",
       "      <th>total_demand_new</th>\n",
       "      <th>total_demand_new_lag1</th>\n",
       "      <th>total_demand_new_lag2</th>\n",
       "      <th>total_demand_new_lag3</th>\n",
       "      <th>total_demand_new_lag-1</th>\n",
       "      <th>total_demand_new_lag-2</th>\n",
       "      <th>total_demand_new_lag-3</th>\n",
       "      <th>smoothed_total_demand</th>\n",
       "      <th>year</th>\n",
       "      <th>total_demand_new_lag4</th>\n",
       "      <th>t4w_total_demand</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>day</th>\n",
       "      <th>t4w_total_demand_lag2</th>\n",
       "      <th>szn_factor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-07-10</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>2156.459303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2110.494687</td>\n",
       "      <td>2194.632691</td>\n",
       "      <td>2186.039578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-10</th>\n",
       "      <td>597</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            zone_id        date  total_demand_new  total_demand_new_lag1  \\\n",
       "date_idx                                                                   \n",
       "2017-07-10        1  2017-07-10       2156.459303                    NaN   \n",
       "2017-07-10      597  2017-07-10          0.000000                    NaN   \n",
       "\n",
       "            total_demand_new_lag2  total_demand_new_lag3  \\\n",
       "date_idx                                                   \n",
       "2017-07-10                    NaN                    NaN   \n",
       "2017-07-10                    NaN                    NaN   \n",
       "\n",
       "            total_demand_new_lag-1  total_demand_new_lag-2  \\\n",
       "date_idx                                                     \n",
       "2017-07-10             2110.494687             2194.632691   \n",
       "2017-07-10                0.000000                0.000000   \n",
       "\n",
       "            total_demand_new_lag-3  smoothed_total_demand  year  \\\n",
       "date_idx                                                          \n",
       "2017-07-10             2186.039578                    NaN  2017   \n",
       "2017-07-10                0.000000                    NaN  2017   \n",
       "\n",
       "            total_demand_new_lag4  t4w_total_demand  month  week  day  \\\n",
       "date_idx                                                                \n",
       "2017-07-10                    NaN               NaN      7    28   10   \n",
       "2017-07-10                    NaN               NaN      7    28   10   \n",
       "\n",
       "            t4w_total_demand_lag2  szn_factor  \n",
       "date_idx                                       \n",
       "2017-07-10                    NaN         NaN  \n",
       "2017-07-10                    NaN         NaN  "
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('weekly_dmd_new.csv')\n",
    "df['date'] = pd.to_datetime(df.date).apply(lambda x: x.date())\n",
    "df = df.set_index(df.date)\n",
    "df.index.rename('date_idx', inplace=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "cdca01bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_start_idx = date(2020,2,17)\n",
    "\n",
    "df_prep = df.loc[(df.date < covid_start_idx)\n",
    "                | (df.date >= covid_start_idx + timedelta(weeks=52))].sort_values('date')\n",
    "\n",
    "df_prep['y_obs'] = df_prep.total_demand_new\n",
    "# df_prep['y_obs'] = df_prep.smoothed_total_demand\n",
    "df_prep['covid_start_m1'] = df_prep.date.apply(lambda x: 1.0 if x == covid_start_idx - timedelta(weeks=1) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1edd543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [] remove first X (3?) weeks for each zone?\n",
    "# [] how to align ish\n",
    "    # should be aligned temporally\n",
    "    # can access the season estimates based on time t then?\n",
    "    # will have to do a theano.scan for each of the zones -- for zones that started after the oldest zone, do they not get initial seasons then? are their initial seasons just the 52 prior \n",
    "\n",
    "# maybe initial level and initial trend can be unique to each zone, but other params come from a shared distribution\n",
    "\n",
    "# so the \"parent\" distribution for initial szns should be ordered/accessed based on time\n",
    "\n",
    "# so s'pose we have\n",
    "# szn_len = 4\n",
    "#  t: 0,1,2,3,4,5,...\n",
    "# z1: 3,4,8,2,5,6,...\n",
    "# z2: _,_,_,1,3,4,...\n",
    "\n",
    "# initial_szns = s1,s2,s3,s4\n",
    "\n",
    "# the first non-zero entry for z2 is the 4th season\n",
    "# so the initial_szns passed to z2's theano.scan should be ordered as [s4,s1,s2,s3]\n",
    "\n",
    "# s1_mu ~ Uniform[0,1]\n",
    "# s1_sigma ~ HalfCauchy(0.5)\n",
    "# s1_mu_z1 ~ TruncatedNormal(s1_mu, s1_sigma, upper=1, lower=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_list = dd.zone_id.unique()\n",
    "dd['zone_idx'] = dd.zone_id.apply(lambda x: np.where(zone_list == x)[0][0])\n",
    "\n",
    "# does this need to be mapped to [0,len(dd.zone_id.unique())-1] ? seems yes\n",
    "zone_idx = dd.zone_idx.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b292e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a102fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_hw_model(zdf, initial_lvl_upper):\n",
    "    szn_len = 52\n",
    "\n",
    "    with pm.Model() as hw_model:\n",
    "    #     initial_level = pm.Normal(f'initial_level', mu=3000, sigma=400)\n",
    "#         initial_level = pm.Uniform(f'initial_level', lower=0, upper=5000)\n",
    "        initial_level = pm.Uniform(f'initial_level', lower=0, upper=initial_lvl_upper, shape=(df_prep.zone_id.nunique()))\n",
    "    \n",
    "        smoothing_level_mu = pm.Uniform('smoothing_level_mu', lower=0, upper=1)\n",
    "        smoothing_level_sigma = pm.HalfCauchy('smoothing_level_sigma', beta=0.5)\n",
    "        smoothing_level = pm.TruncatedNormal('smoothing_level', mu=smoothing_level_mu, sigma=smoothing_level_sigma, lower=0, upper=1, shape=(df_prep.zone_id.nunique()))\n",
    "#         smoothing_level = pm.Normal('smoothing_level', mu=0.5, sigma=0.5)\n",
    "\n",
    "    #     initial_trend = pm.Normal(f'initial_trend', mu=0, sigma=100)\n",
    "#         initial_trend = pm.Uniform(f'initial_trend', lower=-1000, upper=1000)\n",
    "        initial_trend = pm.Uniform(f'initial_trend', lower=-initial_lvl_upper/2, upper=initial_lvl_upper/2)\n",
    "        \n",
    "        smoothing_trend_mu = pm.Uniform('smoothing_trend_mu', lower=0, upper=1)\n",
    "        smoothing_trend_sigma = pm.HalfCauchy('smoothing_trend_sigma', beta=0.5)\n",
    "        smoothing_trend = pm.TruncatedNormal('smoothing_trend', mu=smoothing_trend_mu, sigma=smoothing_trend_sigma, lower=0, upper=1, shape=(df_prep.zone_id.nunique()))\n",
    "#         smoothing_trend = pm.Uniform('smoothing_trend', lower=0, upper=1)\n",
    "    #     smoothing_trend = pm.Normal('smoothing_trend', mu=0.5, sigma=0.5)\n",
    "    \n",
    "        \n",
    "        # \n",
    "        initial_szns_mus = pm.Normal('initial_szns_mus', mu=1.0, sigma=0.2, shape=(szn_len))\n",
    "        initial_szns = pm.Normal('initial_szns', mu=initial_szns_mus, sigma=0.2, shape=(df_prep.zone_id.nunique(), szn_len))\n",
    "    #     initial_szns = pm.Uniform('initial_szns', lower=0.7, upper=1.3, shape=(szn_len))\n",
    "    #     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1)\n",
    "    #     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1)\n",
    "    #     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1-smoothing_level)\n",
    "        smoothing_season_mu = pm.Uniform('smoothing_season_mu', lower=0, upper=1)\n",
    "        smoothing_season_sigma = pm.HalfCauchy('smoothing_season_sigma', beta=0.5)\n",
    "        smoothing_season = pm.TruncatedNormal('smoothing_season', mu=smoothing_season_mu, sigma=smoothing_season_sigma, lower=0, upper=1, shape=(df_prep.zone_id.nunique()))\n",
    "#         smoothing_season = pm.TruncatedNormal('smoothing_season', mu=0.2, sigma=0.5, lower=0, upper=0.8)\n",
    "    #     smoothing_season = pm.Normal('smoothing_season', mu=0.5, sigma=0.5)\n",
    "\n",
    "        ys = tt.as_tensor_variable(zdf['y_obs'])\n",
    "        is_covid_start = tt.as_tensor_variable(zdf['covid_start_m1'])\n",
    "    #     covid_level_change = pm.Normal('covid_level_change', mu=15000, sigma=2000)\n",
    "        covid_level_change = pm.Uniform('covid_level_change', lower=0, upper=5000)\n",
    "#         covid_level_change = pm.Normal('covid_level_change', mu=2000, sigma=1000)\n",
    "    #     covid_level_change2 = pm.Normal('covid_level_change2', mu=20000, sigma=2000)\n",
    "        covid_level_change2 = pm.Uniform('covid_level_change2', lower=0, upper=5000)\n",
    "    #     covid_level_change2 = pm.Normal('covid_level_change2', mu=covid_level_change, sigma=1000)\n",
    "    #     covid_level_change2 = pm.Normal('covid_level_change2', mu=covid_level_change, sigma=1000)\n",
    "\n",
    "        def hw_component_wise(y, covid_start, prior_level, prior_trend, szn_m52):\n",
    "            updated_szn = smoothing_season * y / (prior_level + prior_trend) + (1 - smoothing_season) * szn_m52\n",
    "\n",
    "\n",
    "            next_level = smoothing_level * (y/szn_m52) + (1 - smoothing_level)*(prior_level + prior_trend) + covid_start*covid_level_change\n",
    "            next_trend = smoothing_trend * (next_level - prior_level) + (1 - smoothing_trend)*prior_trend - covid_start*smoothing_trend*covid_level_change2\n",
    "\n",
    "\n",
    "            return next_level, next_trend, updated_szn\n",
    "\n",
    "        # pass initial_level[zone_idx], initial_trend[zone_idx], initial_szns[zone_idx] etc\n",
    "        # should the scan be wrapped in a scan too??\n",
    "            # the outer scan would loop through each zone and pass each zone's initial params and data to the inner scan that calls hw_component_wise\n",
    "        outputs, updates = theano.scan(hw_component_wise,\n",
    "               sequences = [\n",
    "                   ys[1:],\n",
    "#                    ys[szn_len:],\n",
    "                   is_covid_start[1:]\n",
    "#                    is_covid_start[szn_len:]\n",
    "        #            post_covid[szn_len:]\n",
    "               ],\n",
    "               outputs_info = [\n",
    "                   dict(initial = initial_level, taps=None),\n",
    "                   dict(initial = initial_trend, taps=None),\n",
    "                   dict(initial = initial_szns, taps=[-szn_len])\n",
    "               ], \n",
    "        #            non_sequences = \n",
    "               )\n",
    "\n",
    "\n",
    "        levels = outputs[0]\n",
    "        trends = outputs[1]\n",
    "        seasons = outputs[2]\n",
    "\n",
    "\n",
    "        levels_f = pm.math.concatenate([initial_level.reshape(1,1), levels[:-1]])\n",
    "        trends_f = pm.math.concatenate([initial_trend.reshape(1,1), trends[:-1]])\n",
    "        seasons_f = pm.math.concatenate([initial_szns, seasons[:-szn_len]])\n",
    "\n",
    "        levels_and_trends = pm.math.stack([levels_f, trends_f])\n",
    "        level_plus_trend = levels_and_trends.sum(axis=0)\n",
    "\n",
    "        level_plus_trend_and_seasons = pm.math.stack([level_plus_trend, seasons_f])\n",
    "        y_hats = level_plus_trend_and_seasons.prod(axis=0)\n",
    "\n",
    "        sig = pm.HalfCauchy('sigma', beta=10)\n",
    "#         y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=zdf.iloc[szn_len-1:-1, :]['y_obs_lag-1'])\n",
    "        y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=zdf.iloc[:-1, :]['y_obs_lag-1'])\n",
    "        \n",
    "    map_estimate = pm.find_MAP(model=hw_model)\n",
    "    \n",
    "    return map_estimate, ys, is_covid_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "32feaa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = df_prep.loc[df_prep.zone_id.isin([1,13])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "ab65548b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-323-8e0f7511f979>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  d2['zone_idx'] = d2.zone_id.apply(lambda x: np.where(zone_list == x)[0][0])\n"
     ]
    }
   ],
   "source": [
    "zone_list = d2.zone_id.unique()\n",
    "d2['zone_idx'] = d2.zone_id.apply(lambda x: np.where(zone_list == x)[0][0])\n",
    "\n",
    "# does this need to be mapped to [0,len(dd.zone_id.unique())-1] ? seems yes\n",
    "zone_idx = d2.zone_idx.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "d3138d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zone_id</th>\n",
       "      <th>date</th>\n",
       "      <th>total_demand_new</th>\n",
       "      <th>total_demand_new_lag1</th>\n",
       "      <th>total_demand_new_lag2</th>\n",
       "      <th>total_demand_new_lag3</th>\n",
       "      <th>total_demand_new_lag-1</th>\n",
       "      <th>total_demand_new_lag-2</th>\n",
       "      <th>total_demand_new_lag-3</th>\n",
       "      <th>smoothed_total_demand</th>\n",
       "      <th>...</th>\n",
       "      <th>total_demand_new_lag4</th>\n",
       "      <th>t4w_total_demand</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>day</th>\n",
       "      <th>t4w_total_demand_lag2</th>\n",
       "      <th>szn_factor</th>\n",
       "      <th>y_obs</th>\n",
       "      <th>covid_start_m1</th>\n",
       "      <th>zone_idx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-07-10</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>2156.459303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2110.494687</td>\n",
       "      <td>2194.632691</td>\n",
       "      <td>2186.039578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2156.459303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-10</th>\n",
       "      <td>13</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>353.463568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>345.466023</td>\n",
       "      <td>350.206164</td>\n",
       "      <td>360.568384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>353.463568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-17</th>\n",
       "      <td>13</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>345.466023</td>\n",
       "      <td>353.463568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350.206164</td>\n",
       "      <td>360.568384</td>\n",
       "      <td>349.628004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>345.466023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-17</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>2110.494687</td>\n",
       "      <td>2156.459303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2194.632691</td>\n",
       "      <td>2186.039578</td>\n",
       "      <td>2171.274071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2110.494687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-24</th>\n",
       "      <td>13</td>\n",
       "      <td>2017-07-24</td>\n",
       "      <td>350.206164</td>\n",
       "      <td>345.466023</td>\n",
       "      <td>353.463568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.568384</td>\n",
       "      <td>349.628004</td>\n",
       "      <td>352.812292</td>\n",
       "      <td>352.281495</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350.206164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            zone_id        date  total_demand_new  total_demand_new_lag1  \\\n",
       "date_idx                                                                   \n",
       "2017-07-10        1  2017-07-10       2156.459303                    NaN   \n",
       "2017-07-10       13  2017-07-10        353.463568                    NaN   \n",
       "2017-07-17       13  2017-07-17        345.466023             353.463568   \n",
       "2017-07-17        1  2017-07-17       2110.494687            2156.459303   \n",
       "2017-07-24       13  2017-07-24        350.206164             345.466023   \n",
       "\n",
       "            total_demand_new_lag2  total_demand_new_lag3  \\\n",
       "date_idx                                                   \n",
       "2017-07-10                    NaN                    NaN   \n",
       "2017-07-10                    NaN                    NaN   \n",
       "2017-07-17                    NaN                    NaN   \n",
       "2017-07-17                    NaN                    NaN   \n",
       "2017-07-24             353.463568                    NaN   \n",
       "\n",
       "            total_demand_new_lag-1  total_demand_new_lag-2  \\\n",
       "date_idx                                                     \n",
       "2017-07-10             2110.494687             2194.632691   \n",
       "2017-07-10              345.466023              350.206164   \n",
       "2017-07-17              350.206164              360.568384   \n",
       "2017-07-17             2194.632691             2186.039578   \n",
       "2017-07-24              360.568384              349.628004   \n",
       "\n",
       "            total_demand_new_lag-3  smoothed_total_demand  ...  \\\n",
       "date_idx                                                   ...   \n",
       "2017-07-10             2186.039578                    NaN  ...   \n",
       "2017-07-10              360.568384                    NaN  ...   \n",
       "2017-07-17              349.628004                    NaN  ...   \n",
       "2017-07-17             2171.274071                    NaN  ...   \n",
       "2017-07-24              352.812292             352.281495  ...   \n",
       "\n",
       "            total_demand_new_lag4  t4w_total_demand  month  week  day  \\\n",
       "date_idx                                                                \n",
       "2017-07-10                    NaN               NaN      7    28   10   \n",
       "2017-07-10                    NaN               NaN      7    28   10   \n",
       "2017-07-17                    NaN               NaN      7    29   17   \n",
       "2017-07-17                    NaN               NaN      7    29   17   \n",
       "2017-07-24                    NaN               NaN      7    30   24   \n",
       "\n",
       "            t4w_total_demand_lag2  szn_factor        y_obs  covid_start_m1  \\\n",
       "date_idx                                                                     \n",
       "2017-07-10                    NaN         NaN  2156.459303             0.0   \n",
       "2017-07-10                    NaN         NaN   353.463568             0.0   \n",
       "2017-07-17                    NaN         NaN   345.466023             0.0   \n",
       "2017-07-17                    NaN         NaN  2110.494687             0.0   \n",
       "2017-07-24                    NaN         NaN   350.206164             0.0   \n",
       "\n",
       "            zone_idx  \n",
       "date_idx              \n",
       "2017-07-10         0  \n",
       "2017-07-10         1  \n",
       "2017-07-17         1  \n",
       "2017-07-17         0  \n",
       "2017-07-24         1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "5000edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test that we can do the hierarchical scanning business\n",
    "szn_len = 4\n",
    "d = pd.DataFrame({'zone_id': [1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2],\n",
    "#                   'y_obs': [1,2,3,4,2,3,4,5,3,1,2,3,4,1],\n",
    "                  'y_obs': [1,2,3,4,2,3,4,5,3,0,0,0,0,1,2,3,4,1],\n",
    "                  't': [1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,9]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "0d58f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_list = d.zone_id.unique()\n",
    "d['zone_idx'] = d.zone_id.apply(lambda x: np.where(zone_list == x)[0][0])\n",
    "d['covid_start_m1'] = d.t.apply(lambda x: 1 if x == 7 else 0)\n",
    "d['date'] = d.t\n",
    "\n",
    "# does this need to be mapped to [0,len(dd.zone_id.unique())-1] ? seems yes\n",
    "zone_idx = d.zone_idx.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "98e6cff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zone_id</th>\n",
       "      <th>y_obs</th>\n",
       "      <th>t</th>\n",
       "      <th>zone_idx</th>\n",
       "      <th>covid_start_m1</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    zone_id  y_obs  t  zone_idx  covid_start_m1  date\n",
       "0         1      1  1         0               0     1\n",
       "1         1      2  2         0               0     2\n",
       "2         1      3  3         0               0     3\n",
       "3         1      4  4         0               0     4\n",
       "4         1      2  5         0               0     5\n",
       "5         1      3  6         0               0     6\n",
       "6         1      4  7         0               1     7\n",
       "7         1      5  8         0               0     8\n",
       "8         1      3  9         0               0     9\n",
       "9         2      0  1         1               0     1\n",
       "10        2      0  2         1               0     2\n",
       "11        2      0  3         1               0     3\n",
       "12        2      0  4         1               0     4\n",
       "13        2      1  5         1               0     5\n",
       "14        2      2  6         1               0     6\n",
       "15        2      3  7         1               1     7\n",
       "16        2      4  8         1               0     8\n",
       "17        2      1  9         1               0     9"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cfb20af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(zone_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a35cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the \"parent\" distribution for initial szns should be ordered/accessed based on time\n",
    "\n",
    "# so s'pose we have\n",
    "# szn_len = 4\n",
    "#  t: 0,1,2,3,4,5,...\n",
    "# z1: 3,4,8,2,5,6,...\n",
    "# z2: _,_,_,1,3,4,...\n",
    "\n",
    "# initial_szns = s1,s2,s3,s4\n",
    "\n",
    "# the first non-zero entry for z2 is the 4th season\n",
    "# so the initial_szns passed to z2's theano.scan should be ordered as [s4,s1,s2,s3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w/ what ive been doing, we havent worried about which szn corresponds to which t, we just start from wherever there is demand (or 3rd week w/ demand)\n",
    "# now though, we should always index on a particular t/week #\n",
    "# ie make initial_szns[:, 0] correspond to first week in year, initial_szns[:, 51] corresponding to last, etc\n",
    "# so given the first_t, we want:\n",
    "\n",
    "initial_szns[z_idx, first_t%52:szn_len] + initial_szns[z_idx, :first_t%52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6efe918",
   "metadata": {},
   "outputs": [],
   "source": [
    "theano.config.compute_test_value = 'raise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f10f8532",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# def test(x, prev_output):\n",
    "def test(x):\n",
    "    return pm.math.floatX(x**2)\n",
    "\n",
    "outputs, updates = theano.scan(test,\n",
    "                               sequences=[\n",
    "                                   np.array([1.0,2.0,3.0,4.0])\n",
    "#                                    tt.as_tensor_variable([1.0,2.0,3.0,4.0])\n",
    "                               ],\n",
    "#                               outputs_info=[\n",
    "#                                   dict(initial=pm.math.floatX(0), taps=None)\n",
    "#                               ]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "d3593bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.scan.basic): Output None (index 0) has a initial state but taps is explicitly set to None \n",
      "WARNING (theano.scan.basic): Output None (index 1) has a initial state but taps is explicitly set to None \n"
     ]
    }
   ],
   "source": [
    "## test data\n",
    "\n",
    "with pm.Model() as test_hw_model:\n",
    "#     initial_level = pm.Normal(f'initial_level', mu=3000, sigma=400)\n",
    "#         initial_level = pm.Uniform(f'initial_level', lower=0, upper=5000)\n",
    "    initial_lvl_upper = 10000\n",
    "    initial_level = pm.Uniform(f'initial_level', lower=0, upper=initial_lvl_upper, shape=(d.zone_id.nunique()))\n",
    "\n",
    "    smoothing_level_mu = pm.Uniform('smoothing_level_mu', lower=0, upper=1)\n",
    "    smoothing_level_sigma = pm.HalfCauchy('smoothing_level_sigma', beta=0.5)\n",
    "    smoothing_level = pm.TruncatedNormal('smoothing_level', mu=smoothing_level_mu, sigma=smoothing_level_sigma, lower=0, upper=1, shape=(d.zone_id.nunique()))\n",
    "#         smoothing_level = pm.Normal('smoothing_level', mu=0.5, sigma=0.5)\n",
    "\n",
    "#     initial_trend = pm.Normal(f'initial_trend', mu=0, sigma=100)\n",
    "#         initial_trend = pm.Uniform(f'initial_trend', lower=-1000, upper=1000)\n",
    "    initial_trend = pm.Uniform(f'initial_trend', lower=-initial_lvl_upper/2, upper=initial_lvl_upper/2, shape=(d.zone_id.nunique()))\n",
    "\n",
    "    smoothing_trend_mu = pm.Uniform('smoothing_trend_mu', lower=0, upper=1)\n",
    "    smoothing_trend_sigma = pm.HalfCauchy('smoothing_trend_sigma', beta=0.5)\n",
    "    smoothing_trend = pm.TruncatedNormal('smoothing_trend', mu=smoothing_trend_mu, sigma=smoothing_trend_sigma, lower=0, upper=1, shape=(d.zone_id.nunique()))\n",
    "#         smoothing_trend = pm.Uniform('smoothing_trend', lower=0, upper=1)\n",
    "#     smoothing_trend = pm.Normal('smoothing_trend', mu=0.5, sigma=0.5)\n",
    "\n",
    "\n",
    "    # \n",
    "    initial_szns_mus = pm.Normal('initial_szns_mus', mu=1.0, sigma=0.2, shape=(szn_len))\n",
    "    initial_szns = pm.Normal('initial_szns', mu=initial_szns_mus, sigma=0.2, shape=(d.zone_id.nunique(), szn_len))\n",
    "#     initial_szns = pm.Uniform('initial_szns', lower=0.7, upper=1.3, shape=(szn_len))\n",
    "#     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1)\n",
    "#     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1)\n",
    "#     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1-smoothing_level)\n",
    "    smoothing_season_mu = pm.Uniform('smoothing_season_mu', lower=0, upper=1)\n",
    "    smoothing_season_sigma = pm.HalfCauchy('smoothing_season_sigma', beta=0.5)\n",
    "    smoothing_season = pm.TruncatedNormal('smoothing_season', mu=smoothing_season_mu, sigma=smoothing_season_sigma, lower=0, upper=1, shape=(d.zone_id.nunique()))\n",
    "#         smoothing_season = pm.TruncatedNormal('smoothing_season', mu=0.2, sigma=0.5, lower=0, upper=0.8)\n",
    "#     smoothing_season = pm.Normal('smoothing_season', mu=0.5, sigma=0.5)\n",
    "\n",
    "#     ys = tt.as_tensor_variable(zdf['y_obs'])\n",
    "#     is_covid_start = tt.as_tensor_variable(zdf['covid_start_m1'])\n",
    "\n",
    "#     covid_level_change = pm.Normal('covid_level_change', mu=15000, sigma=2000)\n",
    "    covid_level_change = pm.Uniform('covid_level_change', lower=0, upper=5000, shape=(d.zone_id.nunique()))\n",
    "#         covid_level_change = pm.Normal('covid_level_change', mu=2000, sigma=1000)\n",
    "#     covid_level_change2 = pm.Normal('covid_level_change2', mu=20000, sigma=2000)\n",
    "    covid_level_change2 = pm.Uniform('covid_level_change2', lower=0, upper=5000, shape=(d.zone_id.nunique()))\n",
    "    \n",
    "    def hw_component_wise_wrapper(z_idx):\n",
    "        def hw_component_wise(y, covid_start, prior_level, prior_trend, szn_m52):\n",
    "            updated_szn = smoothing_season[z_idx] * y / (prior_level + prior_trend) + (1 - smoothing_season[z_idx]) * szn_m52\n",
    "\n",
    "\n",
    "            next_level = smoothing_level[z_idx] * (y/szn_m52) + (1 - smoothing_level[z_idx])*(prior_level + prior_trend) + covid_start*covid_level_change[z_idx]\n",
    "            next_trend = smoothing_trend[z_idx] * (next_level - prior_level) + (1 - smoothing_trend[z_idx])*prior_trend - covid_start*smoothing_trend[z_idx]*covid_level_change2[z_idx]\n",
    "\n",
    "\n",
    "            return next_level, next_trend, updated_szn\n",
    "        return hw_component_wise\n",
    "    \n",
    "    def inner_scan_for_zone(z_idx, ys, is_covid_start):\n",
    "        outputs, updates = theano.scan(hw_component_wise_wrapper(z_idx),\n",
    "                                      sequences=[\n",
    "                                          ys[1:], # this [1:] is done assuming we padded and shifted y_obs\n",
    "                                          is_covid_start[1:]\n",
    "                                      ],\n",
    "                                      outputs_info=[\n",
    "                                          dict(initial = initial_level[z_idx], taps=None),\n",
    "                                          dict(initial = initial_trend[z_idx], taps=None),\n",
    "                                          dict(initial = initial_szns[z_idx], taps=[-szn_len])\n",
    "    #                                       dict(initial = initial_seasons, taps=[-szn_len])\n",
    "                                      ])\n",
    "        return outputs\n",
    "\n",
    "    outputs, updates = theano.scan(inner_scan_for_zone,\n",
    "               sequences=[\n",
    "                   tt.as_tensor_variable(np.unique(zone_idx)),\n",
    "    #                np.unique(zone_idx),\n",
    "                   tt.as_tensor_variable(ys_list),\n",
    "    #                np.array(ys_list),\n",
    "                   tt.as_tensor_variable(is_covid_start_list),\n",
    "    #                np.array(is_covid_start_list),\n",
    "    #                np.array(first_t_list)\n",
    "               ])\n",
    "    \n",
    "    \n",
    "    ## [] adjust below to account for the now multi-zone setup\n",
    "#     levels = outputs[0]\n",
    "#     trends = outputs[1]\n",
    "#     seasons = outputs[2]\n",
    "\n",
    "\n",
    "#     levels_f = pm.math.concatenate([initial_level.reshape(1,1), levels[:-1]])\n",
    "#     trends_f = pm.math.concatenate([initial_trend.reshape(1,1), trends[:-1]])\n",
    "#     seasons_f = pm.math.concatenate([initial_szns, seasons[:-szn_len]])\n",
    "\n",
    "#     levels_and_trends = pm.math.stack([levels_f, trends_f])\n",
    "#     level_plus_trend = levels_and_trends.sum(axis=0)\n",
    "\n",
    "#     level_plus_trend_and_seasons = pm.math.stack([level_plus_trend, seasons_f])\n",
    "#     y_hats = level_plus_trend_and_seasons.prod(axis=0)\n",
    "\n",
    "#     sig = pm.HalfCauchy('sigma', beta=10)\n",
    "# #         y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=zdf.iloc[szn_len-1:-1, :]['y_obs_lag-1'])\n",
    "#     y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=zdf.iloc[:-1, :]['y_obs_lag-1'])\n",
    "\n",
    "    levels = outputs[0]\n",
    "    trends = outputs[1]\n",
    "    seasons = outputs[2]\n",
    "\n",
    "    levels_f = pm.math.concatenate([initial_level.reshape((d.zone_id.nunique(),1)), levels[:,:-1]], axis=1)\n",
    "    trends_f = pm.math.concatenate([initial_trend.reshape((d.zone_id.nunique(),1)), trends[:,:-1]], axis=1)\n",
    "    seasons_f = pm.math.concatenate([initial_szns, seasons[:,:-szn_len]], axis=1)\n",
    "\n",
    "    levels_and_trends = pm.math.stack([levels_f, trends_f], axis=1)\n",
    "    level_plus_trend = levels_and_trends.sum(axis=1)\n",
    "\n",
    "    level_plus_trend_and_seasons = pm.math.stack([level_plus_trend, seasons_f], axis=1)\n",
    "    y_hats = level_plus_trend_and_seasons.prod(axis=1)\n",
    "\n",
    "    sig = pm.HalfCauchy('sigma', beta=10)\n",
    "    y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=np.array(y_obs_lagm1_list)[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "2e5c49d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.scan.basic): Output None (index 0) has a initial state but taps is explicitly set to None \n",
      "WARNING (theano.scan.basic): Output None (index 1) has a initial state but taps is explicitly set to None \n"
     ]
    }
   ],
   "source": [
    "## real data\n",
    "\n",
    "with pm.Model() as test_hw_model:\n",
    "#     initial_level = pm.Normal(f'initial_level', mu=3000, sigma=400)\n",
    "#         initial_level = pm.Uniform(f'initial_level', lower=0, upper=5000)\n",
    "    initial_lvl_upper = 10000\n",
    "    initial_level = pm.Uniform(f'initial_level', lower=0, upper=initial_lvl_upper, shape=(d2.zone_id.nunique()))\n",
    "\n",
    "    smoothing_level_mu = pm.Uniform('smoothing_level_mu', lower=0, upper=1)\n",
    "    smoothing_level_sigma = pm.HalfCauchy('smoothing_level_sigma', beta=0.5)\n",
    "    smoothing_level = pm.TruncatedNormal('smoothing_level', mu=smoothing_level_mu, sigma=smoothing_level_sigma, lower=0, upper=1, shape=(d2.zone_id.nunique()))\n",
    "#         smoothing_level = pm.Normal('smoothing_level', mu=0.5, sigma=0.5)\n",
    "\n",
    "#     initial_trend = pm.Normal(f'initial_trend', mu=0, sigma=100)\n",
    "#         initial_trend = pm.Uniform(f'initial_trend', lower=-1000, upper=1000)\n",
    "    initial_trend = pm.Uniform(f'initial_trend', lower=-initial_lvl_upper/2, upper=initial_lvl_upper/2, shape=(d2.zone_id.nunique()))\n",
    "\n",
    "    smoothing_trend_mu = pm.Uniform('smoothing_trend_mu', lower=0, upper=1)\n",
    "    smoothing_trend_sigma = pm.HalfCauchy('smoothing_trend_sigma', beta=0.5)\n",
    "    smoothing_trend = pm.TruncatedNormal('smoothing_trend', mu=smoothing_trend_mu, sigma=smoothing_trend_sigma, lower=0, upper=1, shape=(d2.zone_id.nunique()))\n",
    "#         smoothing_trend = pm.Uniform('smoothing_trend', lower=0, upper=1)\n",
    "#     smoothing_trend = pm.Normal('smoothing_trend', mu=0.5, sigma=0.5)\n",
    "\n",
    "\n",
    "    # \n",
    "    initial_szns_mus = pm.Normal('initial_szns_mus', mu=1.0, sigma=0.2, shape=(szn_len))\n",
    "    initial_szns = pm.Normal('initial_szns', mu=initial_szns_mus, sigma=0.2, shape=(d2.zone_id.nunique(), szn_len))\n",
    "#     initial_szns = pm.Uniform('initial_szns', lower=0.7, upper=1.3, shape=(szn_len))\n",
    "#     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1)\n",
    "#     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1)\n",
    "#     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1-smoothing_level)\n",
    "    smoothing_season_mu = pm.Uniform('smoothing_season_mu', lower=0, upper=1)\n",
    "    smoothing_season_sigma = pm.HalfCauchy('smoothing_season_sigma', beta=0.5)\n",
    "    smoothing_season = pm.TruncatedNormal('smoothing_season', mu=smoothing_season_mu, sigma=smoothing_season_sigma, lower=0, upper=1, shape=(d2.zone_id.nunique()))\n",
    "#         smoothing_season = pm.TruncatedNormal('smoothing_season', mu=0.2, sigma=0.5, lower=0, upper=0.8)\n",
    "#     smoothing_season = pm.Normal('smoothing_season', mu=0.5, sigma=0.5)\n",
    "\n",
    "#     ys = tt.as_tensor_variable(zdf['y_obs'])\n",
    "#     is_covid_start = tt.as_tensor_variable(zdf['covid_start_m1'])\n",
    "\n",
    "#     covid_level_change = pm.Normal('covid_level_change', mu=15000, sigma=2000)\n",
    "    covid_level_change = pm.Uniform('covid_level_change', lower=0, upper=5000, shape=(d2.zone_id.nunique()))\n",
    "#         covid_level_change = pm.Normal('covid_level_change', mu=2000, sigma=1000)\n",
    "#     covid_level_change2 = pm.Normal('covid_level_change2', mu=20000, sigma=2000)\n",
    "    covid_level_change2 = pm.Uniform('covid_level_change2', lower=0, upper=5000, shape=(d2.zone_id.nunique()))\n",
    "    \n",
    "    def hw_component_wise_wrapper(z_idx):\n",
    "        def hw_component_wise(y, covid_start, prior_level, prior_trend, szn_m52):\n",
    "            updated_szn = smoothing_season[z_idx] * y / (prior_level + prior_trend) + (1 - smoothing_season[z_idx]) * szn_m52\n",
    "\n",
    "\n",
    "            next_level = smoothing_level[z_idx] * (y/szn_m52) + (1 - smoothing_level[z_idx])*(prior_level + prior_trend) + covid_start*covid_level_change[z_idx]\n",
    "            next_trend = smoothing_trend[z_idx] * (next_level - prior_level) + (1 - smoothing_trend[z_idx])*prior_trend - covid_start*smoothing_trend[z_idx]*covid_level_change2[z_idx]\n",
    "\n",
    "\n",
    "            return next_level, next_trend, updated_szn\n",
    "        return hw_component_wise\n",
    "    \n",
    "    def inner_scan_for_zone(z_idx, ys, is_covid_start):\n",
    "        outputs, updates = theano.scan(hw_component_wise_wrapper(z_idx),\n",
    "                                      sequences=[\n",
    "                                          ys[1:], # this [1:] is done assuming we padded and shifted y_obs\n",
    "                                          is_covid_start[1:]\n",
    "                                      ],\n",
    "                                      outputs_info=[\n",
    "                                          dict(initial = initial_level[z_idx], taps=None),\n",
    "                                          dict(initial = initial_trend[z_idx], taps=None),\n",
    "                                          dict(initial = initial_szns[z_idx], taps=[-szn_len])\n",
    "    #                                       dict(initial = initial_seasons, taps=[-szn_len])\n",
    "                                      ])\n",
    "        return outputs\n",
    "\n",
    "    outputs, updates = theano.scan(inner_scan_for_zone,\n",
    "               sequences=[\n",
    "                   tt.as_tensor_variable(np.unique(zone_idx)),\n",
    "    #                np.unique(zone_idx),\n",
    "                   tt.as_tensor_variable(ys_list),\n",
    "    #                np.array(ys_list),\n",
    "                   tt.as_tensor_variable(is_covid_start_list),\n",
    "    #                np.array(is_covid_start_list),\n",
    "    #                np.array(first_t_list)\n",
    "               ])\n",
    "    \n",
    "    \n",
    "    ## [] adjust below to account for the now multi-zone setup\n",
    "#     levels = outputs[0]\n",
    "#     trends = outputs[1]\n",
    "#     seasons = outputs[2]\n",
    "\n",
    "\n",
    "#     levels_f = pm.math.concatenate([initial_level.reshape(1,1), levels[:-1]])\n",
    "#     trends_f = pm.math.concatenate([initial_trend.reshape(1,1), trends[:-1]])\n",
    "#     seasons_f = pm.math.concatenate([initial_szns, seasons[:-szn_len]])\n",
    "\n",
    "#     levels_and_trends = pm.math.stack([levels_f, trends_f])\n",
    "#     level_plus_trend = levels_and_trends.sum(axis=0)\n",
    "\n",
    "#     level_plus_trend_and_seasons = pm.math.stack([level_plus_trend, seasons_f])\n",
    "#     y_hats = level_plus_trend_and_seasons.prod(axis=0)\n",
    "\n",
    "#     sig = pm.HalfCauchy('sigma', beta=10)\n",
    "# #         y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=zdf.iloc[szn_len-1:-1, :]['y_obs_lag-1'])\n",
    "#     y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=zdf.iloc[:-1, :]['y_obs_lag-1'])\n",
    "\n",
    "    levels = outputs[0]\n",
    "    trends = outputs[1]\n",
    "    seasons = outputs[2]\n",
    "\n",
    "    levels_f = pm.math.concatenate([initial_level.reshape((d2.zone_id.nunique(),1)), levels[:,:-1]], axis=1)\n",
    "    trends_f = pm.math.concatenate([initial_trend.reshape((d2.zone_id.nunique(),1)), trends[:,:-1]], axis=1)\n",
    "    seasons_f = pm.math.concatenate([initial_szns, seasons[:,:-szn_len]], axis=1)\n",
    "\n",
    "    levels_and_trends = pm.math.stack([levels_f, trends_f], axis=1)\n",
    "    level_plus_trend = levels_and_trends.sum(axis=1)\n",
    "\n",
    "    level_plus_trend_and_seasons = pm.math.stack([level_plus_trend, seasons_f], axis=1)\n",
    "    y_hats = level_plus_trend_and_seasons.prod(axis=1)\n",
    "\n",
    "    sig = pm.HalfCauchy('sigma', beta=10)\n",
    "    y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=np.array(y_obs_lagm1_list)[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "1bedace7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.opt): Optimization Warning: The Op erfcx does not provide a C implementation. As well as being potentially slow, this also disables loop fusion.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1692' class='' max='1692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1692/1692 00:50<00:00 logp = -2,132.8, ||grad|| = 66,632]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "map_estimate = pm.find_MAP(model=test_hw_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "a5dff01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hats.get_test_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "65ebf52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_obs_lagm1_list)[:,:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "c04b022c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(zone_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "32060b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[nan, 1, 2, 3, 4, 2, 3, 4, 5, 3], [nan, 0, 0, 0, 0, 1, 2, 3, 4, 1]]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "e95b21c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[for{cpu,scan_fn}.0, for{cpu,scan_fn}.1, for{cpu,scan_fn}.2]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "511ee987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "for{cpu,scan_fn}.0"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "26467a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].get_test_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "1b9c5c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2500.5       ,  626.375     , -466.28125   , -894.7890625 ,\n",
       "        -884.34610305, -656.04268706, 2124.6349552 , 1111.81977946,\n",
       "         330.96402298],\n",
       "       [2500.        ,  625.        , -468.75      , -898.4375    ,\n",
       "        -887.671875  , -658.87109375, 2122.24707031, 1109.74438477,\n",
       "         326.05919889]])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].get_test_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# levels = outputs[0] has shape (n_zones, len(data))\n",
    "# want each zone's initial_level to be concatenated with its remaining levels, save for the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "22c1a75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5000.        , 2500.5       ,  626.375     , -466.28125   ,\n",
       "        -894.7890625 , -884.34610305, -656.04268706, 2124.6349552 ,\n",
       "        1111.81977946],\n",
       "       [5000.        , 2500.        ,  625.        , -468.75      ,\n",
       "        -898.4375    , -887.671875  , -658.87109375, 2122.24707031,\n",
       "        1109.74438477]])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm.math.concatenate([initial_level.reshape((2,1)), levels[:,:-1]], axis=1).get_test_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "3ac96224",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = outputs[0]\n",
    "trends = outputs[1]\n",
    "seasons = outputs[2]\n",
    "\n",
    "\n",
    "levels_f = pm.math.concatenate([initial_level.reshape((d.zone_id.nunique(),1)), levels[:,:-1]], axis=1)\n",
    "trends_f = pm.math.concatenate([initial_trend.reshape((d.zone_id.nunique(),1)), trends[:,:-1]], axis=1)\n",
    "seasons_f = pm.math.concatenate([initial_szns, seasons[:,:-szn_len]], axis=1)\n",
    "\n",
    "levels_and_trends = pm.math.stack([levels_f, trends_f], axis=1)\n",
    "level_plus_trend = levels_and_trends.sum(axis=1)\n",
    "\n",
    "level_plus_trend_and_seasons = pm.math.stack([level_plus_trend, seasons_f], axis=1)\n",
    "y_hats = level_plus_trend_and_seasons.prod(axis=1)\n",
    "\n",
    "sig = pm.HalfCauchy('sigma', beta=10)\n",
    "#         y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=zdf.iloc[szn_len-1:-1, :]['y_obs_lag-1'])\n",
    "y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=np.array(y_obs_lagm1_list)[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "6ef49ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5000.        ,  1250.75      ,  -935.5625    , -1793.578125  ,\n",
       "         -886.52297227,  -660.09172594,  -378.1613891 ,  1104.34022251,\n",
       "          162.14170503],\n",
       "       [ 5000.        ,  1250.        ,  -937.5       , -1796.875     ,\n",
       "         -888.671875  ,  -660.87109375,  -380.75292969,  1105.74438477,\n",
       "          161.84614636]])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hats.get_test_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "22e6b35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hats.get_test_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "c501b80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.,  2.,  3.,  4.,  5.,  3., nan],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  2.,  3.,  4.,  1., nan]])"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_obs_lagm1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "91f4cad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4., 2., 3., 4., 5., 3.],\n",
       "       [0., 0., 0., 0., 1., 2., 3., 4., 1.]])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_obs_lagm1_list)[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "1190f493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_obs_lagm1_list)[:,:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = outputs[0]\n",
    "trends = outputs[1]\n",
    "seasons = outputs[2]\n",
    "\n",
    "\n",
    "levels_f = pm.math.concatenate([initial_level.reshape((d.zone_id.nunique(),1)), levels[:-1]])\n",
    "trends_f = pm.math.concatenate([initial_trend.reshape((d.zone_id.nunique(),1)), trends[:-1]])\n",
    "seasons_f = pm.math.concatenate([initial_szns, seasons[:-szn_len]])\n",
    "\n",
    "levels_and_trends = pm.math.stack([levels_f, trends_f])\n",
    "level_plus_trend = levels_and_trends.sum(axis=0)\n",
    "\n",
    "level_plus_trend_and_seasons = pm.math.stack([level_plus_trend, seasons_f])\n",
    "y_hats = level_plus_trend_and_seasons.prod(axis=0)\n",
    "\n",
    "sig = pm.HalfCauchy('sigma', beta=10)\n",
    "#         y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=zdf.iloc[szn_len-1:-1, :]['y_obs_lag-1'])\n",
    "y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=zdf.iloc[:-1, :]['y_obs_lag-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "9e7fda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prep data\n",
    "ys_list, is_covid_start_list, first_t_list, y_obs_lagm1_list =  [], [], [], []\n",
    "\n",
    "for z_idx in np.unique(zone_idx):\n",
    "    zdf = d.loc[d.zone_idx == z_idx].sort_values('date')\n",
    "    z_id = zdf.zone_id.unique().item()\n",
    "    \n",
    "    pad_df = pd.DataFrame(columns = zdf.columns)\n",
    "    pad_df.loc[0, 'zone_id'] = z_id\n",
    "    pad_df.loc[0, 'zone_idx'] = z_idx\n",
    "    zdf = pd.concat([pad_df, zdf])\n",
    "    zdf['y_obs_lag-1'] = zdf.y_obs.shift(-1)\n",
    "    \n",
    "    ys_list.append(zdf['y_obs'].tolist())\n",
    "    y_obs_lagm1_list.append(zdf['y_obs_lag-1'].tolist())\n",
    "    is_covid_start_list.append(zdf['covid_start_m1'].tolist())\n",
    "    first_t_list.append(zdf.t.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "4ec2a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prep data\n",
    "# ys_list, is_covid_start_list, first_t_list =  [], [], []\n",
    "szn_len = 52\n",
    "ys_list, is_covid_start_list, y_obs_lagm1_list =  [], [], []\n",
    "\n",
    "for z_idx in np.unique(zone_idx):\n",
    "    zdf = d2.loc[d2.zone_idx == z_idx].sort_values('date')\n",
    "    z_id = zdf.zone_id.unique().item()\n",
    "    \n",
    "    pad_df = pd.DataFrame(columns = zdf.columns)\n",
    "    pad_df.loc[0, 'zone_id'] = z_id\n",
    "    pad_df.loc[0, 'zone_idx'] = z_idx\n",
    "    zdf = pd.concat([pad_df, zdf])\n",
    "    zdf['y_obs_lag-1'] = zdf.y_obs.shift(-1)\n",
    "    \n",
    "    ys_list.append(zdf['y_obs'].tolist())\n",
    "    y_obs_lagm1_list.append(zdf['y_obs_lag-1'].tolist())\n",
    "    is_covid_start_list.append(zdf['covid_start_m1'].tolist())\n",
    "#     first_t_list.append(zdf.t.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4d51d83e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# def test(x, prev_output):\n",
    "def test(x,y,o1,o2,o3):\n",
    "#     return pm.math.floatX(x**2), pm.math.floatX(3.0)\n",
    "    return pm.math.floatX(x**2), pm.math.floatX(x**2), pm.math.floatX(x**2)\n",
    "\n",
    "# outputs, updates = theano.scan(test,\n",
    "#                                sequences=[\n",
    "#                                    np.array([1.0,2.0,3.0,4.0])\n",
    "# #                                    tt.as_tensor_variable([1.0,2.0,3.0,4.0])\n",
    "#                                ],\n",
    "# #                               outputs_info=[\n",
    "# #                                   dict(initial=pm.math.floatX(0), taps=None)\n",
    "# #                               ]\n",
    "#                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "1c02b72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothing_level.get_test_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "b394eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hw_component_wise_wrapper(z_idx):\n",
    "    def hw_component_wise(y, covid_start, prior_level, prior_trend, szn_m52):\n",
    "        updated_szn = smoothing_season[z_idx] * y / (prior_level + prior_trend) + (1 - smoothing_season[z_idx]) * szn_m52\n",
    "\n",
    "\n",
    "        next_level = smoothing_level[z_idx] * (y/szn_m52) + (1 - smoothing_level[z_idx])*(prior_level + prior_trend) + covid_start*covid_level_change[z_idx]\n",
    "        next_trend = smoothing_trend[z_idx] * (next_level - prior_level) + (1 - smoothing_trend[z_idx])*prior_trend - covid_start*smoothing_trend[z_idx]*covid_level_change2[z_idx]\n",
    "\n",
    "\n",
    "        return next_level, next_trend, updated_szn\n",
    "    return hw_component_wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "d1927fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.scan.basic): Output None (index 0) has a initial state but taps is explicitly set to None \n",
      "WARNING (theano.scan.basic): Output None (index 1) has a initial state but taps is explicitly set to None \n"
     ]
    }
   ],
   "source": [
    "# def inner_scan_for_zone(zone_idx, ys, is_covid_start, first_t):\n",
    "def inner_scan_for_zone(z_idx, ys, is_covid_start):\n",
    "    outputs, updates = theano.scan(hw_component_wise_wrapper(z_idx),\n",
    "                                  sequences=[\n",
    "                                      ys[1:], # this [1:] is done assuming we padded and shifted y_obs\n",
    "                                      is_covid_start[1:]\n",
    "                                  ],\n",
    "                                  outputs_info=[\n",
    "                                      dict(initial = initial_level[z_idx], taps=None),\n",
    "                                      dict(initial = initial_trend[z_idx], taps=None),\n",
    "                                      dict(initial = initial_szns[z_idx], taps=[-szn_len])\n",
    "#                                       dict(initial = initial_seasons, taps=[-szn_len])\n",
    "                                  ])\n",
    "    return outputs\n",
    "\n",
    "outputs, updates = theano.scan(inner_scan_for_zone,\n",
    "           sequences=[\n",
    "               tt.as_tensor_variable(np.unique(zone_idx)),\n",
    "#                np.unique(zone_idx),\n",
    "               tt.as_tensor_variable(ys_list),\n",
    "#                np.array(ys_list),\n",
    "               tt.as_tensor_variable(is_covid_start_list),\n",
    "#                np.array(is_covid_start_list),\n",
    "#                np.array(first_t_list)\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a0ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, we have the hierarchical priors, etc\n",
    "# we have the inner functions\n",
    "# just need to make sure df_prep is setup correctly, etc per below and test\n",
    "\n",
    "# [] bring back the initial_level_upper thing we had for the prior? maybe need to do per zone\n",
    "# [] make sure we correctly align outputs and observeds\n",
    "\n",
    "# [] apply the padding for ys[1:], etc\n",
    "# [] confirm ok that we're padding newer zones with 0's (initial level will be 0 i guess, but should be fine -- since the optimal smoothing_level will still be determined by the non-0 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "1f8ec970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[for{cpu,scan_fn}.0, for{cpu,scan_fn}.1, for{cpu,scan_fn}.2]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "9c95b41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2501.        ,  627.25      , -465.1875    , -895.109375  ,\n",
       "        -883.79416827, 1845.30301136,  876.55212782,  173.53125729],\n",
       "       [2500.        ,  625.        , -468.75      , -897.9375    ,\n",
       "        -886.296875  , 1843.09765625,  874.52050781,  170.60236347]])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].get_test_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "315b7cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  9., 16.,  4.,  9., 16., 25.,  9.],\n",
       "       [ 0.,  0.,  0.,  1.,  4.,  9., 16.,  1.]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[2].get_test_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d546c903",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-111-1840a6545aa7>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-111-1840a6545aa7>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    return outputs\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "for z_idx in np.unique(zone_idx):\n",
    "    zdf = d2.loc[d2.zone_idx == zone_idx]\n",
    "    ys = tt.as_tensor_variable(zdf['y_obs'])\n",
    "    is_covid_start = tt.as_tensor_variable(zdf['covid_start_m1'])\n",
    "    first_t = zdf.t.min()\n",
    "    initial_seasons = initial_szns[zone_idx, first_t%szn_len:szn_len] + initial_szns[zone_idx, :first_t%szn_len]\n",
    "    \n",
    "    outputs, updates = theano.scan(hw_component_wise,\n",
    "               sequences = [\n",
    "                   ys[1:],\n",
    "#                    ys[szn_len:],\n",
    "                   is_covid_start[1:]\n",
    "#                    is_covid_start[szn_len:]\n",
    "        #            post_covid[szn_len:]\n",
    "               ],\n",
    "               outputs_info = [\n",
    "                   dict(initial = initial_level[zone_idx], taps=None),\n",
    "                   dict(initial = initial_trend[zone_idx], taps=None),\n",
    "                   dict(initial = initial_seasons, taps=[-szn_len])\n",
    "#                    dict(initial = initial_szns[z_idx], taps=[-szn_len])\n",
    "               ], \n",
    "        #            non_sequences = \n",
    "               )\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c92ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_hw_model(zdf, initial_lvl_upper):\n",
    "    szn_len = 52\n",
    "\n",
    "    with pm.Model() as hw_model:\n",
    "    #     initial_level = pm.Normal(f'initial_level', mu=3000, sigma=400)\n",
    "#         initial_level = pm.Uniform(f'initial_level', lower=0, upper=5000)\n",
    "        initial_level = pm.Uniform(f'initial_level', lower=0, upper=initial_lvl_upper, shape=(df_prep.zone_id.nunique()))\n",
    "    \n",
    "        smoothing_level_mu = pm.Uniform('smoothing_level_mu', lower=0, upper=1)\n",
    "        smoothing_level_sigma = pm.HalfCauchy('smoothing_level_sigma', beta=0.5)\n",
    "        smoothing_level = pm.TruncatedNormal('smoothing_level', mu=smoothing_level_mu, sigma=smoothing_level_sigma, lower=0, upper=1, shape=(df_prep.zone_id.nunique()))\n",
    "#         smoothing_level = pm.Normal('smoothing_level', mu=0.5, sigma=0.5)\n",
    "\n",
    "    #     initial_trend = pm.Normal(f'initial_trend', mu=0, sigma=100)\n",
    "#         initial_trend = pm.Uniform(f'initial_trend', lower=-1000, upper=1000)\n",
    "        initial_trend = pm.Uniform(f'initial_trend', lower=-initial_lvl_upper/2, upper=initial_lvl_upper/2)\n",
    "        \n",
    "        smoothing_trend_mu = pm.Uniform('smoothing_trend_mu', lower=0, upper=1)\n",
    "        smoothing_trend_sigma = pm.HalfCauchy('smoothing_trend_sigma', beta=0.5)\n",
    "        smoothing_trend = pm.TruncatedNormal('smoothing_trend', mu=smoothing_trend_mu, sigma=smoothing_trend_sigma, lower=0, upper=1, shape=(df_prep.zone_id.nunique()))\n",
    "#         smoothing_trend = pm.Uniform('smoothing_trend', lower=0, upper=1)\n",
    "    #     smoothing_trend = pm.Normal('smoothing_trend', mu=0.5, sigma=0.5)\n",
    "    \n",
    "        \n",
    "        # \n",
    "        initial_szns_mus = pm.Normal('initial_szns_mus', mu=1.0, sigma=0.2, shape=(szn_len))\n",
    "        initial_szns = pm.Normal('initial_szns', mu=initial_szns_mus, sigma=0.2, shape=(df_prep.zone_id.nunique(), szn_len))\n",
    "    #     initial_szns = pm.Uniform('initial_szns', lower=0.7, upper=1.3, shape=(szn_len))\n",
    "    #     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1)\n",
    "    #     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1)\n",
    "    #     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1-smoothing_level)\n",
    "        smoothing_season_mu = pm.Uniform('smoothing_season_mu', lower=0, upper=1)\n",
    "        smoothing_season_sigma = pm.HalfCauchy('smoothing_season_sigma', beta=0.5)\n",
    "        smoothing_season = pm.TruncatedNormal('smoothing_season', mu=smoothing_season_mu, sigma=smoothing_season_sigma, lower=0, upper=1, shape=(df_prep.zone_id.nunique()))\n",
    "#         smoothing_season = pm.TruncatedNormal('smoothing_season', mu=0.2, sigma=0.5, lower=0, upper=0.8)\n",
    "    #     smoothing_season = pm.Normal('smoothing_season', mu=0.5, sigma=0.5)\n",
    "\n",
    "        ys = tt.as_tensor_variable(zdf['y_obs'])\n",
    "        is_covid_start = tt.as_tensor_variable(zdf['covid_start_m1'])\n",
    "    #     covid_level_change = pm.Normal('covid_level_change', mu=15000, sigma=2000)\n",
    "        covid_level_change = pm.Uniform('covid_level_change', lower=0, upper=5000)\n",
    "#         covid_level_change = pm.Normal('covid_level_change', mu=2000, sigma=1000)\n",
    "    #     covid_level_change2 = pm.Normal('covid_level_change2', mu=20000, sigma=2000)\n",
    "        covid_level_change2 = pm.Uniform('covid_level_change2', lower=0, upper=5000)\n",
    "    #     covid_level_change2 = pm.Normal('covid_level_change2', mu=covid_level_change, sigma=1000)\n",
    "    #     covid_level_change2 = pm.Normal('covid_level_change2', mu=covid_level_change, sigma=1000)\n",
    "\n",
    "        def hw_component_wise(y, covid_start, prior_level, prior_trend, szn_m52):\n",
    "            updated_szn = smoothing_season * y / (prior_level + prior_trend) + (1 - smoothing_season) * szn_m52\n",
    "\n",
    "\n",
    "            next_level = smoothing_level * (y/szn_m52) + (1 - smoothing_level)*(prior_level + prior_trend) + covid_start*covid_level_change\n",
    "            next_trend = smoothing_trend * (next_level - prior_level) + (1 - smoothing_trend)*prior_trend - covid_start*smoothing_trend*covid_level_change2\n",
    "\n",
    "\n",
    "            return next_level, next_trend, updated_szn\n",
    "\n",
    "        # pass initial_level[zone_idx], initial_trend[zone_idx], initial_szns[zone_idx] etc\n",
    "        # should the scan be wrapped in a scan too??\n",
    "            # the outer scan would loop through each zone and pass each zone's initial params and data to the inner scan that calls hw_component_wise\n",
    "        outputs, updates = theano.scan(hw_component_wise,\n",
    "               sequences = [\n",
    "                   ys[1:],\n",
    "#                    ys[szn_len:],\n",
    "                   is_covid_start[1:]\n",
    "#                    is_covid_start[szn_len:]\n",
    "        #            post_covid[szn_len:]\n",
    "               ],\n",
    "               outputs_info = [\n",
    "                   dict(initial = initial_level, taps=None),\n",
    "                   dict(initial = initial_trend, taps=None),\n",
    "                   dict(initial = initial_szns, taps=[-szn_len])\n",
    "               ], \n",
    "        #            non_sequences = \n",
    "               )\n",
    "\n",
    "\n",
    "        levels = outputs[0]\n",
    "        trends = outputs[1]\n",
    "        seasons = outputs[2]\n",
    "\n",
    "\n",
    "        levels_f = pm.math.concatenate([initial_level.reshape(1,1), levels[:-1]])\n",
    "        trends_f = pm.math.concatenate([initial_trend.reshape(1,1), trends[:-1]])\n",
    "        seasons_f = pm.math.concatenate([initial_szns, seasons[:-szn_len]])\n",
    "\n",
    "        levels_and_trends = pm.math.stack([levels_f, trends_f])\n",
    "        level_plus_trend = levels_and_trends.sum(axis=0)\n",
    "\n",
    "        level_plus_trend_and_seasons = pm.math.stack([level_plus_trend, seasons_f])\n",
    "        y_hats = level_plus_trend_and_seasons.prod(axis=0)\n",
    "\n",
    "        sig = pm.HalfCauchy('sigma', beta=10)\n",
    "#         y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=zdf.iloc[szn_len-1:-1, :]['y_obs_lag-1'])\n",
    "        y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=zdf.iloc[:-1, :]['y_obs_lag-1'])\n",
    "        \n",
    "    map_estimate = pm.find_MAP(model=hw_model)\n",
    "    \n",
    "    return map_estimate, ys, is_covid_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c289e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34a438e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22121a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dampened\n",
    "\n",
    "# when training, should we train on the horizon of interest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0757fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_hw_model(zdf, initial_lvl_upper):\n",
    "    szn_len = 52\n",
    "\n",
    "    with pm.Model() as hw_model:\n",
    "    #     initial_level = pm.Normal(f'initial_level', mu=3000, sigma=400)\n",
    "#         initial_level = pm.Uniform(f'initial_level', lower=0, upper=5000)\n",
    "        initial_level = pm.Uniform(f'initial_level', lower=0, upper=initial_lvl_upper)\n",
    "        smoothing_level = pm.Uniform('smoothing_level', lower=0, upper=1)\n",
    "#         smoothing_level = pm.Normal('smoothing_level', mu=0.5, sigma=0.5)\n",
    "\n",
    "    #     initial_trend = pm.Normal(f'initial_trend', mu=0, sigma=100)\n",
    "#         initial_trend = pm.Uniform(f'initial_trend', lower=-1000, upper=1000)\n",
    "        initial_trend = pm.Uniform(f'initial_trend', lower=-initial_lvl_upper/2, upper=initial_lvl_upper/2)\n",
    "        smoothing_trend = pm.Uniform('smoothing_trend', lower=0, upper=1)\n",
    "    #     smoothing_trend = pm.Normal('smoothing_trend', mu=0.5, sigma=0.5)\n",
    "\n",
    "        initial_szns = pm.Normal('initial_szns', mu=1.0, sigma=0.2, shape=(szn_len))\n",
    "    #     initial_szns = pm.Uniform('initial_szns', lower=0.7, upper=1.3, shape=(szn_len))\n",
    "    #     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1)\n",
    "    #     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1)\n",
    "    #     smoothing_season = pm.Uniform('smoothing_season', lower=0, upper=1-smoothing_level)\n",
    "        smoothing_season = pm.TruncatedNormal('smoothing_season', mu=0.2, sigma=0.5, lower=0, upper=0.8)\n",
    "    #     smoothing_season = pm.Normal('smoothing_season', mu=0.5, sigma=0.5)\n",
    "    \n",
    "        dampener = pm.Uniform('dampener', lower=0, upper=1)\n",
    "\n",
    "        ys = tt.as_tensor_variable(zdf['y_obs'])\n",
    "        is_covid_start = tt.as_tensor_variable(zdf['covid_start_m1'])\n",
    "    #     covid_level_change = pm.Normal('covid_level_change', mu=15000, sigma=2000)\n",
    "        covid_level_change = pm.Uniform('covid_level_change', lower=0, upper=5000)\n",
    "#         covid_level_change = pm.Normal('covid_level_change', mu=2000, sigma=1000)\n",
    "    #     covid_level_change2 = pm.Normal('covid_level_change2', mu=20000, sigma=2000)\n",
    "        covid_level_change2 = pm.Uniform('covid_level_change2', lower=0, upper=5000)\n",
    "    #     covid_level_change2 = pm.Normal('covid_level_change2', mu=covid_level_change, sigma=1000)\n",
    "    #     covid_level_change2 = pm.Normal('covid_level_change2', mu=covid_level_change, sigma=1000)\n",
    "\n",
    "        def hw_component_wise(y, covid_start, prior_level, prior_trend, szn_m52):\n",
    "            updated_szn = smoothing_season * y / (prior_level + dampener*prior_trend) + (1 - smoothing_season) * szn_m52\n",
    "\n",
    "\n",
    "            next_level = smoothing_level * (y/szn_m52) + (1 - smoothing_level)*(prior_level + dampener*prior_trend) + covid_start*covid_level_change\n",
    "            next_trend = smoothing_trend * (next_level - prior_level) + (1 - smoothing_trend)*dampener*prior_trend - covid_start*smoothing_trend*covid_level_change2\n",
    "\n",
    "\n",
    "            return next_level, next_trend, updated_szn\n",
    "\n",
    "        outputs, updates = theano.scan(hw_component_wise,\n",
    "               sequences = [\n",
    "                   ys[1:],\n",
    "#                    ys[szn_len:],\n",
    "                   is_covid_start[1:]\n",
    "#                    is_covid_start[szn_len:]\n",
    "        #            post_covid[szn_len:]\n",
    "               ],\n",
    "               outputs_info = [\n",
    "                   dict(initial = initial_level, taps=None),\n",
    "                   dict(initial = initial_trend, taps=None),\n",
    "                   dict(initial = initial_szns, taps=[-szn_len])\n",
    "               ], \n",
    "        #            non_sequences = \n",
    "               )\n",
    "\n",
    "\n",
    "        levels = outputs[0]\n",
    "        trends = outputs[1]\n",
    "        seasons = outputs[2]\n",
    "\n",
    "\n",
    "        levels_f = pm.math.concatenate([initial_level.reshape(1,1), levels[:-1]])\n",
    "        trends_f = pm.math.concatenate([initial_trend.reshape(1,1), trends[:-1]])\n",
    "        trends_f = trends_f * dampener ### pm.math.prod here?\n",
    "        seasons_f = pm.math.concatenate([initial_szns, seasons[:-szn_len]])\n",
    "\n",
    "        levels_and_trends = pm.math.stack([levels_f, trends_f])\n",
    "        level_plus_trend = levels_and_trends.sum(axis=0)\n",
    "\n",
    "        level_plus_trend_and_seasons = pm.math.stack([level_plus_trend, seasons_f])\n",
    "        y_hats = level_plus_trend_and_seasons.prod(axis=0)\n",
    "\n",
    "        sig = pm.HalfCauchy('sigma', beta=10)\n",
    "#         y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=zdf.iloc[szn_len-1:-1, :]['y_obs_lag-1'])\n",
    "        y_like = pm.Normal('y_like', mu=y_hats, sigma=sig, observed=zdf.iloc[:-1, :]['y_obs_lag-1'])\n",
    "        \n",
    "    map_estimate = pm.find_MAP(model=hw_model)\n",
    "    \n",
    "    return map_estimate, ys, is_covid_start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
